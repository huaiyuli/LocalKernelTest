{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Kernel With non-stationary local weight\n",
    "\n",
    "The Gaussian process in consideration is a mixture of independent processes. In specific, there is one global process $\\mathcal{GP}_{g}$ and a collection of local processes $\\{\\mathcal{GP}_{l_i}\\}$. A local non-stationary weight is added to each of the processes, to balance between exploring with global process and exploiting with local processes. The weight is modeled as Gaussian functions, to give the expression below:\n",
    "\\begin{gather*}\n",
    "    f(\\mathbf{x}) = e^{-\\frac{\\lVert \\mathbf{x}-\\pmb{\\psi_g}\\rVert_2^2}{2*\\sigma_g^2}} f_{g}(\\mathbf{x}) + \\sum_i e^{-\\frac{\\lVert \\mathbf{x}-\\pmb{\\psi_l}\\rVert_2^2}{2*\\sigma_{l_i}^2}} f_{l_i}(\\mathbf{x}),\\\\\n",
    "    f_{g} \\sim \\mathcal{GP}_{g},\\quad f_{l_i} \\sim \\mathcal{GP}_{l_i},\n",
    "\\end{gather*}\n",
    "\n",
    "$\\pmb{\\psi}$ denotes the position of the center of the influence region of a process.\n",
    "\n",
    "Additivity of Gaussian processes results in the sum being Gaussian processes. If we further assume no correlation between the $\\mathcal{GP}$ s, then we may describe $\\mathcal{GP}_{tot}:\\ f(\\mathbf{x})\\sim\\mathcal{GP}_{tot}$ uniquely with mean function and covariance kernel as:\n",
    "\n",
    "\\begin{align*}\n",
    "    k(\\mathbf x_1, \\mathbf x_2) = &\\exp\\left(\\frac{\\lVert \\mathbf{x_1}-\\pmb{\\psi_g}\\rVert_2^2 + \\lVert \\mathbf{x_2}-\\pmb{\\psi_g}\\rVert_2^2}{2\\sigma_g^2}\\right)k_g(\\mathbf x_1, \\mathbf x_2)\\\\\n",
    "    &+\\sum_i \\exp\\left(\\frac{\\lVert \\mathbf{x_1}-\\pmb{\\psi_g}\\rVert_2^2 + \\lVert \\mathbf{x_2}-\\pmb{\\psi_g}\\rVert_2^2}{2\\sigma_{l_i}^2}\\right)k_{l_i}(\\mathbf x_1, \\mathbf x_2),\\\\\n",
    "        m(\\mathbf x) = & m_g(\\mathbf{x}) + \\sum_i m_{l_i}(\\mathbf{x}) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Our assumptions are: 1). Local Kernels are at a same place, and the areas of influence are isotropic. 2). input of $\\mathbf x$ is vaguely standardized to $[-1,1]_d$, which can be used for setting the priors of sub-kernel hyperparameters and position/weights hyperparameters. 3). Global kernel has near uniform weight, which can be simulated by placing at $\\pmb{\\psi_g} = [0.5]_d$ and $\\sigma_g$ being large, e.g. taken to be 10. 4). Global weight $\\sigma_g$ isn't a hyperparameter, while local weights $\\sigma_{l_i}$ s are hyperparameters. If necessary, we can use a unified $\\sigma_{l}$. 5). To emphasize the local/global weights, $\\sigma_l \\ll \\sigma_g$ might also be necessary. The practical hyperparameter might be $\\sigma_l/\\sigma_g$, constrained to $(0,1)$, or its logarithm constrained to $(-\\infty,0)$, instead of $\\sigma_l$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.priors import Prior\n",
    "import torch\n",
    "from torch._C import Size\n",
    "from torch.nn import ModuleList\n",
    "from ast import match_case\n",
    "from math import sqrt\n",
    "from statistics import linear_regression\n",
    "from typing import Iterable\n",
    "from gpytorch.kernels import Kernel\n",
    "from linear_operator.operators import ZeroLinearOperator\n",
    "from numpy import iterable\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "# Example from Spartan Kernel\n",
    "\n",
    "class SpartanKernel(Kernel):\n",
    "\n",
    "        has_lengthscale = False\n",
    "\n",
    "        def __init__(self, global_kernel: Kernel, local_kernels: Iterable[Kernel], \n",
    "                     ard_num_dims: int = 1,\n",
    "                     local_position_prior: Optional[Prior] = None,\n",
    "                     local_position_constraint: Optional[Interval] = None,\n",
    "                     eps: float = 0.000001, **kwargs):\n",
    "                \n",
    "                \n",
    "                super(SpartanKernel, self).__init__(ard_num_dims=ard_num_dims)\n",
    "\n",
    "                self.global_kernel = global_kernel\n",
    "                self.local_kernels = ModuleList(local_kernels)\n",
    "                self.register_parameter(\n",
    "                        name=\"raw_local_position\", \n",
    "                        parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape,1,ard_num_dims))\n",
    "                )\n",
    "                if local_position_prior is not None:\n",
    "                        if not isinstance(local_position_prior, Prior):\n",
    "                                raise TypeError(\"Expected gpytorch.priors.Prior but got \" + type(local_position_prior).__name__)\n",
    "                        self.register_prior(\n",
    "                                \"local_position_prior\",\n",
    "                                local_position_prior,\n",
    "                                lambda m: m.local_position,\n",
    "                                lambda m, v: m._set_local_position(v),\n",
    "                        )\n",
    "                if local_position_constraint is None:\n",
    "                        local_position_constraint = Interval(torch.zeros(1, ard_num_dims).squeeze(), \n",
    "                                                             torch.ones(1, ard_num_dims).squeeze())\n",
    "                        # Constrained between 0 to 1, can be modified for the inputs\n",
    "                self.register_constraint(\"raw_local_position\", local_position_constraint)\n",
    "                # Weight parameters other than center:\n",
    "                weight_params = {'psi' : torch.ones([1,ard_num_dims])*0.5,\n",
    "                                 'sigma_g' : sqrt(10.),\n",
    "                                 'Normalized' : True,\n",
    "                                 'sigma_l' : torch.tensor([sqrt(0.01)])}\n",
    "                #                 'local_num_samples' : None}\n",
    "                weight_params.update(kwargs)\n",
    "                self.eps = eps\n",
    "                self.register_buffer('psi', weight_params['psi'])\n",
    "                self.register_buffer('sigma_g', torch.as_tensor(weight_params['sigma_g']))\n",
    "                self.Normalized = weight_params['Normalized']\n",
    "                #if weight_params['local_num_samples'] is not None:\n",
    "                #        self.sigma_l = sqrt(weight_params['local_num_samples']/2)\n",
    "                #else:\n",
    "                self.register_buffer('sigma_l', weight_params['sigma_l'])\n",
    "                # TO DO: What if we want seperate sigma_l for different kernels?\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        @property\n",
    "        def local_position(self):\n",
    "                return self.raw_local_position_constraint.transform(self.raw_local_position)\n",
    "\n",
    "        @local_position.setter\n",
    "        def local_position(self, value):\n",
    "                return self._set_local_position(value)\n",
    "        \n",
    "        def _set_local_position(self, value):\n",
    "                if not torch.is_tensor(value):\n",
    "                        value = torch.as_tensor(value).to(self.raw_local_position)\n",
    "                \n",
    "                self.initialize(raw_local_position = self.raw_local_position_constraint.inverse_transform(value))\n",
    "\n",
    "        def omega_g(self, x):\n",
    "                \"\"\"Helper function for unnormalized weights\"\"\"\n",
    "\n",
    "        def forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: bool=False, **params):\n",
    "                #print(x1.shape,x2.shape)\n",
    "                # This is just to make mll work:\n",
    "                res = ZeroLinearOperator() if not diag else 0\n",
    "                #if diag and torch.equal(x1, x2):\n",
    "                #       return torch.ones(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n",
    "                _k_g = self.global_kernel(x1, x2, diag=diag).to_dense()\n",
    "                _w_g_1 = MultivariateNormal(self.psi, torch.eye(self.ard_num_dims, device=self.device)*self.sigma_g**2).log_prob(x1)\n",
    "                _w_g_2 = MultivariateNormal(self.psi, torch.eye(self.ard_num_dims, device=self.device)*self.sigma_g**2).log_prob(x2)\n",
    "                #print(\"w\",_w_g_1.shape,_w_g_2.shape)\n",
    "                _w_sum_1 = torch.exp(_w_g_1) # keep track of total weights\n",
    "                _w_sum_2 = torch.exp(_w_g_2)\n",
    "                if not diag:\n",
    "                        res = res + torch.mul(_k_g, torch.sqrt(torch.matmul(_w_sum_1.unsqueeze(-1),\n",
    "                                                                     _w_sum_2.unsqueeze(-2)))) # TO DO: decide on whether logsum is needed\n",
    "                else:\n",
    "                        res = res + torch.mul(_k_g, torch.exp(_w_g_1 + _w_g_2))\n",
    "                for _k in self.local_kernels:\n",
    "                        # Only same sigma_l for different local kernels considered\n",
    "                        _w_l_dist = MultivariateNormal(self.local_position, torch.eye(self.ard_num_dims, device=self.device)*self.sigma_l**2)\n",
    "                        _w_l_1 = _w_l_dist.log_prob(x1)\n",
    "                        _w_l_2 = _w_l_dist.log_prob(x2)\n",
    "                        _w_sum_1 = _w_sum_1 + torch.exp(_w_l_1)\n",
    "                        _w_sum_2 = _w_sum_2 + torch.exp(_w_l_2)\n",
    "                        if not diag:\n",
    "                                res = res + torch.mul(_k(x1, x2, diag=diag).to_dense(), \n",
    "                                                torch.matmul(torch.exp(_w_l_1/2).unsqueeze(-1), \n",
    "                                                                torch.exp(_w_l_2/2).unsqueeze(-2)))\n",
    "                        else:\n",
    "                                res = res + torch.mul(_k(x1, x2, diag=diag).to_dense(),\n",
    "                                                                torch.exp(_w_l_1/2 + _w_l_2/2))\n",
    "                        \n",
    "                # Now apply normalization\n",
    "                if not diag:\n",
    "                        res = torch.div(res, torch.unsqueeze(torch.sqrt(_w_sum_1), -1))\n",
    "                        if res.dim() > 2:\n",
    "                                res = torch.div(res, torch.unsqueeze(torch.sqrt(_w_sum_2), -2))\n",
    "                        else:\n",
    "                                res = torch.div(res, torch.sqrt(_w_sum_2))\n",
    "                else:\n",
    "                        res = torch.div(res, torch.sqrt(torch.mul(_w_sum_1, _w_sum_2)))        \n",
    "                return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.priors import Prior\n",
    "import torch\n",
    "from torch._C import Size\n",
    "from torch.nn import ModuleList\n",
    "from ast import match_case\n",
    "from math import sqrt\n",
    "from statistics import linear_regression\n",
    "from typing import Iterable\n",
    "from gpytorch.kernels import Kernel\n",
    "from linear_operator.operators import ZeroLinearOperator\n",
    "from numpy import iterable\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.lazy import ZeroLazyTensor\n",
    "\n",
    "\n",
    "class Gaussian_Weight_Spartan_Kernel(Kernel):\n",
    "        has_lengthscale = False\n",
    "        def __init__(self, global_kernel: Kernel, local_kernels: Iterable[Kernel], \n",
    "                     ard_num_dims: int = 1,\n",
    "                     local_position_prior: Optional[Prior] = None,\n",
    "                     local_position_constraint: Optional[Interval] = None,\n",
    "                     local_weight_var_prior: Optional[Prior] = None,\n",
    "                     local_weight_var_constraint: Optional[Interval] = None,\n",
    "                     eps: float = 0.000001, **kwargs):\n",
    "                \n",
    "                \n",
    "                super().__init__(ard_num_dims=ard_num_dims)\n",
    "\n",
    "                self.global_kernel = global_kernel\n",
    "                self.local_kernels = ModuleList(local_kernels)\n",
    "                # numbers of local kernels for calculation\n",
    "                self.local_kernels_num = len(self.local_kernels)\n",
    "                # hyperparameters for weights\n",
    "                # Note: The logistic functions used to set interval constraints might cause a problem. The optimizer might favour either ends of the interval where the convergence seems to be reached because of almost 0 differentials.\n",
    "                self.register_parameter(\n",
    "                        name=\"raw_local_position\", \n",
    "                        parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape,1,ard_num_dims))\n",
    "                )\n",
    "                if local_position_prior is not None:\n",
    "                        if not isinstance(local_position_prior, Prior):\n",
    "                                raise TypeError(\"Expected gpytorch.priors.Prior but got \" + type(local_position_prior).__name__)\n",
    "                        self.register_prior(\n",
    "                                \"local_position_prior\",\n",
    "                                local_position_prior,\n",
    "                                lambda m: m.local_position,\n",
    "                                lambda m, v: m._set_local_position(v),\n",
    "                        )\n",
    "                if local_position_constraint is None:\n",
    "                        local_position_constraint = Interval(torch.zeros(1, ard_num_dims).squeeze(), \n",
    "                                                             torch.ones(1, ard_num_dims).squeeze())\n",
    "                self.register_constraint(\"raw_local_position\", local_position_constraint)\n",
    "\n",
    "                self.register_parameter(\n",
    "                        name=\"raw_local_weight_var\",\n",
    "                        parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.local_kernels_num))\n",
    "                )\n",
    "                if local_weight_var_prior is not None:\n",
    "                        if not isinstance(local_weight_var_prior, Prior):\n",
    "                                raise TypeError(\"Expected gpytorch.priors.Prior but got \" + type(local_weight_var_prior).__name__)\n",
    "                        self.register_prior(\n",
    "                                \"local_weight_var_prior\",\n",
    "                                local_weight_var_prior,\n",
    "                                lambda m: m.local_weight_var,\n",
    "                                lambda m, v: m._set_local_weight_var(v),\n",
    "                        )\n",
    "                \n",
    "                if local_weight_var_constraint is None:\n",
    "                        local_weight_var_constraint = Interval(torch.zeros(self.local_kernels_num).squeeze(),\n",
    "                                                                torch.ones(self.local_kernels_num).squeeze())       \n",
    "                self.register_constraint(\"raw_local_weight_var\", local_weight_var_constraint)\n",
    "\n",
    "                # Weight parameters other than center:\n",
    "                weight_params = {'psi' : torch.ones([1,ard_num_dims])*0.5,\n",
    "                                 'sigma_g' : sqrt(10.),\n",
    "                                 'Normalized' : True,\n",
    "                                 #'sigma_l' : torch.tensor([sqrt(0.01)])\n",
    "                                 }\n",
    "                #                 'local_num_samples' : None}\n",
    "                weight_params.update(kwargs)\n",
    "                self.eps = eps\n",
    "                self.register_buffer('psi', weight_params['psi'])\n",
    "                self.register_buffer('sigma_g', torch.as_tensor(weight_params['sigma_g']))\n",
    "                self.Normalized = weight_params['Normalized']\n",
    "                #\n",
    "                #if weight_params['local_num_samples'] is not None:\n",
    "                #        self.sigma_l = sqrt(weight_params['local_num_samples']/2)\n",
    "                #else:\n",
    "                self.register_buffer('sigma_l', weight_params['sigma_l'])\n",
    "                # TO DO: Incorporate information on samples for local weight priors?\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        @property\n",
    "        def local_position(self):\n",
    "                return self.raw_local_position_constraint.transform(self.raw_local_position)\n",
    "\n",
    "        @local_position.setter\n",
    "        def local_position(self, value):\n",
    "                return self._set_local_position(value)\n",
    "        \n",
    "        def _set_local_position(self, value):\n",
    "                if not torch.is_tensor(value):\n",
    "                        value = torch.as_tensor(value).to(self.raw_local_position)\n",
    "                \n",
    "                self.initialize(raw_local_position = self.raw_local_position_constraint.inverse_transform(value))\n",
    "\n",
    "        @property\n",
    "        def local_weight_var(self):\n",
    "                return self.raw_local_weight_var_constraint.transform(self.raw_local_weight_var)\n",
    "        \n",
    "        @local_weight_var.setter\n",
    "        def local_weight_var(self, value):\n",
    "                return self._set_local_weight_var(value)\n",
    "        \n",
    "        def _set_local_weight_var(self, value):\n",
    "                if not torch.is_tensor(value):\n",
    "                        value = torch.as_tensor(value).to(self.raw_local_weight_var)\n",
    "\n",
    "                self.initialize(raw_local_weight_var = self.raw_local_weight_var_constraint.inverse_transform(value))\n",
    "\n",
    "        def omega_g(self, x):\n",
    "                \"\"\"Helper function for unnormalized weights\"\"\"\n",
    "\n",
    "        def forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: bool=False, **params):\n",
    "                res = ZeroLazyTensor() if not diag else 0\n",
    "                _k_g = self.global_kernel(x1, x2, diag=diag)\n",
    "                _w_g = (torch.unsqueeze((x1 - self.psi).norm(dim=-1), -1) + torch.unsqueeze((x2 - self.psi).pow(2).norm(dim=-1), -2))/2*self.sigma_g**2\n",
    "                res = res + _k_g.mul(_w_g)\n",
    "                for _kernel, local_var in zip(self.local_kernels, self.local_weight_var):\n",
    "                        _k_l = _kernel(x1, x2, diag = diag)\n",
    "                        _w_l = (torch.unsqueeze((x1 - self.local_position).norm(dim=-1), -1) + torch.unsqueeze((x2 - self.local_position).pow(2).norm(dim=-1), -2))/2*local_var**2\n",
    "                        res = res + _k_l.mul(_w_l)\n",
    "                \n",
    "                return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTorchdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
